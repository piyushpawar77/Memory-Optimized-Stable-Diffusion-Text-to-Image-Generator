{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "MT0RJ5aozVVR",
    "outputId": "59f9a295-829a-4f93-8a55-b193d2c741e8"
   },
   "outputs": [],
   "source": [
    "!pip install diffusers transformers accelerate torch torchvision\n",
    "!pip install gradio safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BrVsVj-rzX_B",
    "outputId": "35e2ae06-5450-4102-feff-37eab6eb43f9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1060
    },
    "id": "6tbJFy3izZ8_",
    "outputId": "b14826fd-62c2-45c6-b23c-539b875adc74"
   },
   "outputs": [],
   "source": [
    "# Stable Diffusion Text-to-Image Generator with Gradio UI\n",
    "# Optimized for Google Colab with Real-time Progress Tracking\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import gradio as gr\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, EulerDiscreteScheduler\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class StableDiffusionGenerator:\n",
    "    def __init__(self):\n",
    "        self.pipe = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.progress_callback = None\n",
    "        self.start_time = None\n",
    "        self.total_steps = 0\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def progress_fn(self, step, timestep, latents):\n",
    "        \"\"\"Callback function to track generation progress\"\"\"\n",
    "        if self.progress_callback and self.start_time:\n",
    "            elapsed_time = time.time() - self.start_time\n",
    "            progress_percent = (step / self.total_steps) * 100\n",
    "\n",
    "            if step > 0:  # Avoid division by zero\n",
    "                estimated_total_time = elapsed_time * self.total_steps / step\n",
    "                remaining_time = estimated_total_time - elapsed_time\n",
    "            else:\n",
    "                remaining_time = 0\n",
    "\n",
    "            # Format time nicely\n",
    "            def format_time(seconds):\n",
    "                if seconds < 60:\n",
    "                    return f\"{seconds:.1f}s\"\n",
    "                elif seconds < 3600:\n",
    "                    return f\"{seconds//60:.0f}m {seconds%60:.0f}s\"\n",
    "                else:\n",
    "                    return f\"{seconds//3600:.0f}h {(seconds%3600)//60:.0f}m\"\n",
    "\n",
    "            progress_text = f\"üé® Generating: {progress_percent:.1f}% complete | \"\n",
    "            progress_text += f\"‚è±Ô∏è Elapsed: {format_time(elapsed_time)} | \"\n",
    "            progress_text += f\"‚è≥ Remaining: {format_time(remaining_time)}\"\n",
    "\n",
    "            # Update progress through callback\n",
    "            self.progress_callback(progress_text)\n",
    "\n",
    "    def load_model(self, model_id=\"runwayml/stable-diffusion-v1-5\", progress=gr.Progress()):\n",
    "        \"\"\"Load the Stable Diffusion model with memory optimizations and progress tracking\"\"\"\n",
    "        try:\n",
    "            # Progress tracking for model loading\n",
    "            progress(0.1, desc=\"üîÑ Clearing previous models...\")\n",
    "\n",
    "            # Clear any existing model from memory\n",
    "            if self.pipe is not None:\n",
    "                del self.pipe\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            progress(0.3, desc=\"üì• Downloading model (this may take a while for first time)...\")\n",
    "            print(f\"Loading model: {model_id}\")\n",
    "\n",
    "            # Load model with memory optimizations\n",
    "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                use_safetensors=True,\n",
    "                variant=\"fp16\" if self.device == \"cuda\" else None,\n",
    "                safety_checker=None,  # Disable safety checker to save memory\n",
    "                requires_safety_checker=False\n",
    "            )\n",
    "\n",
    "            progress(0.7, desc=\"‚öôÔ∏è Optimizing model for Colab...\")\n",
    "\n",
    "            # Use faster, more memory-efficient scheduler\n",
    "            self.pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
    "                self.pipe.scheduler.config\n",
    "            )\n",
    "\n",
    "            # Enable all memory optimizations\n",
    "            if self.device == \"cuda\":\n",
    "                self.pipe.enable_attention_slicing(1)  # More aggressive slicing\n",
    "                self.pipe.enable_vae_slicing()\n",
    "                self.pipe.enable_vae_tiling()  # Additional VAE optimization\n",
    "\n",
    "            progress(0.9, desc=\"üöÄ Moving model to device...\")\n",
    "            self.pipe = self.pipe.to(self.device)\n",
    "\n",
    "            # Enable CPU offloading to save VRAM\n",
    "            if self.device == \"cuda\":\n",
    "                self.pipe.enable_model_cpu_offload()\n",
    "\n",
    "            progress(1.0, desc=\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "            # Clear cache after loading\n",
    "            if self.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"Model loaded successfully!\")\n",
    "            return \"‚úÖ Model loaded successfully! Ready to generate images.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Error loading model: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "\n",
    "    def generate_image(self, prompt, negative_prompt=\"\", num_inference_steps=15,\n",
    "                      guidance_scale=7.5, width=512, height=512, seed=-1,\n",
    "                      progress=gr.Progress()):\n",
    "        \"\"\"Generate image from text prompt with real-time progress\"\"\"\n",
    "\n",
    "        if self.pipe is None:\n",
    "            return None, \"‚ùå Please load a model first!\"\n",
    "\n",
    "        try:\n",
    "            # Validate inputs for optimal Colab performance\n",
    "            if width > 768 or height > 768:\n",
    "                return None, \"‚ùå Image dimensions too large for Colab! Keep width and height ‚â§ 768px\"\n",
    "\n",
    "            if num_inference_steps > 30:\n",
    "                return None, \"‚ùå Too many inference steps! Keep steps ‚â§ 30 for faster generation\"\n",
    "\n",
    "            # Set up progress tracking\n",
    "            self.total_steps = num_inference_steps\n",
    "            self.start_time = time.time()\n",
    "\n",
    "            # Progress callback for Gradio\n",
    "            def update_progress(text):\n",
    "                progress(None, desc=text)\n",
    "\n",
    "            self.progress_callback = update_progress\n",
    "\n",
    "            progress(0.05, desc=\"üé≤ Setting up generation parameters...\")\n",
    "\n",
    "            # Set random seed if specified\n",
    "            if seed != -1:\n",
    "                torch.manual_seed(seed)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.manual_seed(seed)\n",
    "\n",
    "            progress(0.1, desc=\"üßπ Clearing memory cache...\")\n",
    "\n",
    "            # Clear cache before generation\n",
    "            if self.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            progress(0.15, desc=f\"üé® Starting image generation ({num_inference_steps} steps)...\")\n",
    "            print(f\"Generating image for prompt: '{prompt}'\")\n",
    "\n",
    "            # Generate image with progress callback\n",
    "            with torch.autocast(self.device):\n",
    "                result = self.pipe(\n",
    "                    prompt=prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    width=width,\n",
    "                    height=height,\n",
    "                    callback=self.progress_fn,\n",
    "                    callback_steps=1  # Update progress every step\n",
    "                )\n",
    "\n",
    "            image = result.images[0]\n",
    "\n",
    "            progress(0.95, desc=\"üñºÔ∏è Finalizing image...\")\n",
    "\n",
    "            # Clear cache after generation\n",
    "            if self.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            total_time = time.time() - self.start_time\n",
    "            status_msg = f\"‚úÖ Image generated successfully in {total_time:.1f}s!\"\n",
    "            progress(1.0, desc=status_msg)\n",
    "            print(status_msg)\n",
    "            return image, status_msg\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Error generating image: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            progress(1.0, desc=error_msg)\n",
    "            return None, error_msg\n",
    "\n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Get current memory usage information\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            return f\"GPU Memory - Used: {allocated:.2f}GB | Reserved: {reserved:.2f}GB | Total: {total:.1f}GB\"\n",
    "        else:\n",
    "            return \"Running on CPU\"\n",
    "\n",
    "# Initialize the generator\n",
    "generator = StableDiffusionGenerator()\n",
    "\n",
    "# Define the Gradio interface\n",
    "def create_interface():\n",
    "    with gr.Blocks(title=\"Fast Stable Diffusion Generator\", theme=gr.themes.Soft()) as interface:\n",
    "\n",
    "        gr.Markdown(\"# ‚ö° Fast Stable Diffusion Text-to-Image Generator\")\n",
    "        gr.Markdown(\"**Optimized for Google Colab with Real-time Progress Tracking**\")\n",
    "\n",
    "        with gr.Tab(\"üé® Image Generation\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    # Model loading section\n",
    "                    gr.Markdown(\"### 1. Load Model\")\n",
    "                    with gr.Row():\n",
    "                        model_dropdown = gr.Dropdown(\n",
    "                            choices=[\n",
    "                                \"runwayml/stable-diffusion-v1-5\",  # Fastest, most stable\n",
    "                                \"stabilityai/stable-diffusion-2-1-base\",  # Good balance\n",
    "                                \"CompVis/stable-diffusion-v1-4\"  # Alternative option\n",
    "                            ],\n",
    "                            value=\"runwayml/stable-diffusion-v1-5\",\n",
    "                            label=\"Select Model (v1-5 is fastest)\"\n",
    "                        )\n",
    "                        load_btn = gr.Button(\"üöÄ Load Model\", variant=\"primary\", size=\"sm\")\n",
    "\n",
    "                    load_status = gr.Textbox(label=\"Model Status\", interactive=False)\n",
    "\n",
    "                    gr.Markdown(\"### 2. Generation Settings\")\n",
    "                    prompt = gr.Textbox(\n",
    "                        label=\"‚ú® Prompt\",\n",
    "                        placeholder=\"a beautiful sunset over mountains, highly detailed, cinematic lighting\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                    negative_prompt = gr.Textbox(\n",
    "                        label=\"üö´ Negative Prompt (Optional)\",\n",
    "                        placeholder=\"blurry, bad quality, distorted, ugly\",\n",
    "                        lines=2\n",
    "                    )\n",
    "\n",
    "                    with gr.Row():\n",
    "                        steps = gr.Slider(\n",
    "                            minimum=8, maximum=30, value=15, step=1,\n",
    "                            label=\"‚ö° Inference Steps (15 recommended)\",\n",
    "                            info=\"Fewer steps = faster generation\"\n",
    "                        )\n",
    "                        guidance = gr.Slider(\n",
    "                            minimum=3, maximum=15, value=7.5, step=0.5,\n",
    "                            label=\"üéØ Guidance Scale\",\n",
    "                            info=\"Higher = follows prompt more closely\"\n",
    "                        )\n",
    "\n",
    "                    with gr.Row():\n",
    "                        width = gr.Dropdown(\n",
    "                            choices=[256, 384, 512, 640, 768],\n",
    "                            value=512,\n",
    "                            label=\"üìê Width (px)\"\n",
    "                        )\n",
    "                        height = gr.Dropdown(\n",
    "                            choices=[256, 384, 512, 640, 768],\n",
    "                            value=512,\n",
    "                            label=\"üìê Height (px)\"\n",
    "                        )\n",
    "\n",
    "                    seed = gr.Number(\n",
    "                        label=\"üé≤ Seed (-1 for random)\",\n",
    "                        value=-1,\n",
    "                        precision=0,\n",
    "                        info=\"Same seed = same image\"\n",
    "                    )\n",
    "\n",
    "                    generate_btn = gr.Button(\n",
    "                        \"üé® Generate Image\",\n",
    "                        variant=\"primary\",\n",
    "                        size=\"lg\"\n",
    "                    )\n",
    "\n",
    "                    # Quick presets\n",
    "                    gr.Markdown(\"### üöÄ Quick Presets\")\n",
    "                    with gr.Row():\n",
    "                        fast_btn = gr.Button(\"‚ö° Ultra Fast (8 steps)\", size=\"sm\")\n",
    "                        balanced_btn = gr.Button(\"‚öñÔ∏è Balanced (15 steps)\", size=\"sm\")\n",
    "                        quality_btn = gr.Button(\"üé® High Quality (25 steps)\", size=\"sm\")\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### üñºÔ∏è Generated Image\")\n",
    "                    output_image = gr.Image(label=\"Generated Image\", type=\"pil\", height=400)\n",
    "                    generation_status = gr.Textbox(label=\"üìä Generation Status\", interactive=False)\n",
    "\n",
    "                    # Time estimation display\n",
    "                    gr.Markdown(\"### ‚è±Ô∏è Expected Generation Times\")\n",
    "                    gr.HTML(\"\"\"\n",
    "                    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                                padding: 15px; border-radius: 10px; color: white;\">\n",
    "                        <b>Colab Performance (GPU):</b><br>\n",
    "                        ‚Ä¢ 8 steps: ~10-15 seconds<br>\n",
    "                        ‚Ä¢ 15 steps: ~20-30 seconds<br>\n",
    "                        ‚Ä¢ 25 steps: ~40-60 seconds<br>\n",
    "                        <small><i>512x512 resolution</i></small>\n",
    "                    </div>\n",
    "                    \"\"\")\n",
    "\n",
    "        with gr.Tab(\"üìä System Info\"):\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"### üíª System Information\")\n",
    "                memory_info = gr.Textbox(label=\"Memory Usage\", interactive=False)\n",
    "                refresh_memory_btn = gr.Button(\"üîÑ Refresh Memory Info\")\n",
    "\n",
    "                gr.Markdown(\"### ‚ö° Optimization Tips\")\n",
    "                gr.HTML(\"\"\"\n",
    "                <div style=\"background: #f0f8ff; padding: 20px; border-radius: 10px; border-left: 4px solid #4CAF50;\">\n",
    "                    <h4>üöÄ For Fastest Generation:</h4>\n",
    "                    <ul>\n",
    "                        <li><b>Use 8-15 inference steps</b> (sweet spot for speed/quality)</li>\n",
    "                        <li><b>Keep resolution at 512x512</b> (perfect for most uses)</li>\n",
    "                        <li><b>Use runwayml/stable-diffusion-v1-5</b> (fastest model)</li>\n",
    "                        <li><b>Enable GPU in Colab</b> (Runtime ‚Üí Change runtime type ‚Üí GPU)</li>\n",
    "                        <li><b>Clear memory between generations</b> (automatic in this interface)</li>\n",
    "                    </ul>\n",
    "\n",
    "                    <h4>üé® For Best Quality:</h4>\n",
    "                    <ul>\n",
    "                        <li><b>Use detailed, descriptive prompts</b></li>\n",
    "                        <li><b>Add style keywords:</b> \"highly detailed\", \"cinematic\", \"4k\"</li>\n",
    "                        <li><b>Use negative prompts</b> to avoid unwanted elements</li>\n",
    "                        <li><b>Guidance scale 7-8</b> works best for most images</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                \"\"\")\n",
    "\n",
    "        # Event handlers\n",
    "        load_btn.click(\n",
    "            fn=generator.load_model,\n",
    "            inputs=[model_dropdown],\n",
    "            outputs=[load_status],\n",
    "            show_progress=True\n",
    "        )\n",
    "\n",
    "        generate_btn.click(\n",
    "            fn=generator.generate_image,\n",
    "            inputs=[prompt, negative_prompt, steps, guidance, width, height, seed],\n",
    "            outputs=[output_image, generation_status],\n",
    "            show_progress=True\n",
    "        )\n",
    "\n",
    "        # Quick preset handlers\n",
    "        def set_ultra_fast():\n",
    "            return 8, 7.0, 512, 512\n",
    "\n",
    "        def set_balanced():\n",
    "            return 15, 7.5, 512, 512\n",
    "\n",
    "        def set_high_quality():\n",
    "            return 25, 8.0, 640, 640\n",
    "\n",
    "        fast_btn.click(\n",
    "            fn=set_ultra_fast,\n",
    "            outputs=[steps, guidance, width, height]\n",
    "        )\n",
    "\n",
    "        balanced_btn.click(\n",
    "            fn=set_balanced,\n",
    "            outputs=[steps, guidance, width, height]\n",
    "        )\n",
    "\n",
    "        quality_btn.click(\n",
    "            fn=set_high_quality,\n",
    "            outputs=[steps, guidance, width, height]\n",
    "        )\n",
    "\n",
    "        refresh_memory_btn.click(\n",
    "            fn=generator.get_memory_info,\n",
    "            outputs=[memory_info]\n",
    "        )\n",
    "\n",
    "        # Load initial memory info\n",
    "        interface.load(\n",
    "            fn=generator.get_memory_info,\n",
    "            outputs=[memory_info]\n",
    "        )\n",
    "\n",
    "    return interface\n",
    "\n",
    "# Optimized installation instructions for Colab\n",
    "def print_installation_instructions():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ FAST SETUP FOR GOOGLE COLAB\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"üìã Copy and run these commands in separate Colab cells:\")\n",
    "    print()\n",
    "    print(\"# ‚öôÔ∏è Cell 1: Install dependencies (run once)\")\n",
    "    print(\"!pip install -q diffusers[torch] transformers accelerate\")\n",
    "    print(\"!pip install -q gradio safetensors\")\n",
    "    print()\n",
    "    print(\"# üñ•Ô∏è Cell 2: Enable GPU and check status\")\n",
    "    print(\"import torch\")\n",
    "    print(\"print('GPU Available:', torch.cuda.is_available())\")\n",
    "    print(\"if torch.cuda.is_available():\")\n",
    "    print(\"    print('GPU Model:', torch.cuda.get_device_name())\")\n",
    "    print(\"    print('GPU Memory:', f'{torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB')\")\n",
    "    print(\"else:\")\n",
    "    print(\"    print('‚ö†Ô∏è GPU not enabled! Go to Runtime > Change runtime type > Hardware accelerator > GPU')\")\n",
    "    print()\n",
    "    print(\"# üé® Cell 3: Run the generator\")\n",
    "    print(\"# (paste the main script here)\")\n",
    "    print()\n",
    "    print(\"‚è±Ô∏è Expected times with optimizations:\")\n",
    "    print(\"   ‚Ä¢ Model loading: 1-2 minutes (first time only)\")\n",
    "    print(\"   ‚Ä¢ Image generation: 10-60 seconds (depending on settings)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_installation_instructions()\n",
    "\n",
    "    # Create and launch the interface\n",
    "    interface = create_interface()\n",
    "\n",
    "    # Launch with optimized settings for Colab\n",
    "    interface.launch(\n",
    "        share=True,  # Creates public link\n",
    "        inbrowser=True,  # Opens in browser\n",
    "        show_error=True,  # Show errors in interface\n",
    "        quiet=False  # Show progress in console\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
