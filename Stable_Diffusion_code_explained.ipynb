{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT0RJ5aozVVR",
        "outputId": "ae9e0e56-0c47-4ca3-e3b8-83f26165b307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.34.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers transformers accelerate torch torchvision\n",
        "!pip install gradio safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Libraries Used in This Notebook\n",
        "\n",
        "This notebook uses several important libraries to run and optimize **Stable Diffusion** in Google Colab.  \n",
        "Here’s what each one is used for in general, and how it is used in **my code**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 1. `diffusers`\n",
        "Hugging Face’s library dedicated to diffusion models like Stable Diffusion.  \n",
        "It is used in the ML community for text-to-image, image-to-image, inpainting, and other generative tasks.  \n",
        "It provides ready-to-use pipelines and schedulers that simplify running large diffusion models.\n",
        "\n",
        "**In my code:**\n",
        "- used for `StableDiffusionPipeline` to load and run Stable Diffusion.  \n",
        "- used for `EulerDiscreteScheduler` to optimize inference speed in Colab.  \n",
        "- This is the **core library running the model**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 2. `transformers`\n",
        "Hugging Face’s library for natural language processing and multimodal tasks.  \n",
        "It is widely used for pre-trained models like BERT, GPT, and CLIP, which handle text embeddings, classification, and more.  \n",
        "In diffusion workflows, it helps transform text prompts into embeddings the model can understand.\n",
        "\n",
        "**In my code:**\n",
        "- used for the **CLIP text encoder**, which converts prompts into embeddings.  \n",
        "- even though not imported directly, `diffusers` relies on it internally.  \n",
        "- without this library, the model would not understand the input text.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 3. `accelerate`\n",
        "Hugging Face’s library for managing hardware acceleration across CPUs, GPUs, or TPUs.  \n",
        "It is used for distributing models efficiently across devices and simplifying mixed setups (e.g., multiple GPUs).  \n",
        "It abstracts away complex device placement so that developers can focus on the model instead of CUDA details.\n",
        "\n",
        "**In my code:**\n",
        "- used internally by `diffusers` to optimize execution on Colab’s GPU.  \n",
        "- ensures smooth model loading and inference without me handling CUDA directly.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 4. `torch` (PyTorch)\n",
        "A deep learning framework that powers most modern AI models.  \n",
        "It is used for defining neural networks, tensor computations, GPU acceleration, and training/inference of models.  \n",
        "Most generative AI models, including Stable Diffusion, are implemented in PyTorch.\n",
        "\n",
        "**In my code:**\n",
        "- used for GPU detection: `torch.cuda.is_available()`.  \n",
        "- used for memory management: `torch.cuda.empty_cache()`.  \n",
        "- used for reproducibility: `torch.manual_seed()`.  \n",
        "- used for faster inference: `torch.autocast()` (mixed precision).  \n",
        "- all Stable Diffusion model computations run on PyTorch.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 5. `torchvision`\n",
        "An extension of PyTorch focused on computer vision.  \n",
        "It is used for loading image datasets, applying image transformations, and providing pre-trained vision models.  \n",
        "It often comes bundled with PyTorch in ML projects.\n",
        "\n",
        "**In my code:**\n",
        "- used only as a dependency installed with PyTorch.  \n",
        "- not used directly in this notebook.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 6. `gradio`\n",
        "A Python library for creating user interfaces for machine learning models.  \n",
        "It is used widely for building quick demos where users can interact with models through buttons, sliders, textboxes, or images.  \n",
        "This makes complex ML models accessible to non-technical users.\n",
        "\n",
        "**In my code:**\n",
        "- used to build the Colab interface:\n",
        "  - buttons to load models and generate images  \n",
        "  - textboxes for prompts and negative prompts  \n",
        "  - sliders for steps, guidance, and resolution  \n",
        "  - image display for generated results  \n",
        "- used for `gr.Progress()` to show real-time progress during generation.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 7. `safetensors`\n",
        "A specialized file format for storing model weights.  \n",
        "It is used because it loads models faster than `.bin` or `.pt` and avoids security risks from Python pickling.  \n",
        "This format is optimized for large diffusion models.\n",
        "\n",
        "**In my code:**\n",
        "- used for loading Stable Diffusion weights with `use_safetensors=True`.  \n",
        "- used to speed up model loading and save VRAM in Colab.  \n",
        "- ensures safe and efficient checkpoint handling.\n"
      ],
      "metadata": {
        "id": "YZxSVzQPEgka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import torch\n",
        "import gradio as gr\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, EulerDiscreteScheduler\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "3l_n5eYh9qBA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔧 Other Imports in This Notebook\n",
        "\n",
        "Besides the main ML libraries, this notebook also imports standard Python utilities and additional tools that support Stable Diffusion.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `os`\n",
        "The built-in Python library for interacting with the operating system.  \n",
        "It is used for handling file paths, environment variables, and directories.  \n",
        "This is common in ML projects when saving models, outputs, or configuring GPU settings.\n",
        "\n",
        "**In my code:**\n",
        "- used for managing file paths and directories when handling images or checkpoints.  \n",
        "- sometimes used for setting environment variables (e.g., `os.environ[\"CUDA_VISIBLE_DEVICES\"]`).  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `gc`\n",
        "The Python garbage collection library.  \n",
        "It is used for manually cleaning up memory that is no longer needed.  \n",
        "This is important in GPU-heavy tasks like Stable Diffusion, where unused objects can quickly fill VRAM.\n",
        "\n",
        "**In my code:**\n",
        "- used for calling `gc.collect()` after generating images.  \n",
        "- helps free up CPU memory and prevent crashes in Colab.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `time`\n",
        "The built-in Python library for handling time-related tasks.  \n",
        "It is used in ML workflows for logging execution time, creating delays, or measuring performance.\n",
        "\n",
        "**In my code:**\n",
        "- used for tracking how long it takes to generate an image.  \n",
        "- useful for performance optimization and monitoring generation speed.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `StableDiffusionPipeline`, `DPMSolverMultistepScheduler`, `EulerDiscreteScheduler`\n",
        "These come from the `diffusers` library.  \n",
        "They are used for running Stable Diffusion pipelines and choosing different schedulers (the mathematical methods that control how the image denoising process progresses).\n",
        "\n",
        "**In my code:**\n",
        "- used for `StableDiffusionPipeline` → loads and runs the Stable Diffusion model.  \n",
        "- used for `EulerDiscreteScheduler` → provides a fast and memory-efficient denoising scheduler (great for Colab).  \n",
        "- used for `DPMSolverMultistepScheduler` → another scheduler option that can improve image quality at fewer steps.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `PIL.Image`\n",
        "Part of the Python Imaging Library (PIL), now maintained as **Pillow**.  \n",
        "It is used for opening, editing, and saving images in Python.  \n",
        "This is a standard tool in computer vision pipelines.\n",
        "\n",
        "**In my code:**\n",
        "- used for displaying generated Stable Diffusion outputs.  \n",
        "- used for saving results as `.png` or `.jpg` files.  \n",
        "- sometimes used for resizing or converting image formats.\n"
      ],
      "metadata": {
        "id": "BAbBeF0lFQww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StableDiffusionGenerator:\n",
        "    def __init__(self):\n",
        "        self.pipe = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.progress_callback = None\n",
        "        self.start_time = None\n",
        "        self.total_steps = 0\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def progress_fn(self, step, timestep, latents):\n",
        "        \"\"\"Callback function to track generation progress\"\"\"\n",
        "        if self.progress_callback and self.start_time:\n",
        "            elapsed_time = time.time() - self.start_time\n",
        "            progress_percent = (step / self.total_steps) * 100\n",
        "\n",
        "            if step > 0:  # Avoid division by zero\n",
        "                estimated_total_time = elapsed_time * self.total_steps / step\n",
        "                remaining_time = estimated_total_time - elapsed_time\n",
        "            else:\n",
        "                remaining_time = 0\n",
        "\n",
        "            # Format time nicely\n",
        "            def format_time(seconds):\n",
        "                if seconds < 60:\n",
        "                    return f\"{seconds:.1f}s\"\n",
        "                elif seconds < 3600:\n",
        "                    return f\"{seconds//60:.0f}m {seconds%60:.0f}s\"\n",
        "                else:\n",
        "                    return f\"{seconds//3600:.0f}h {(seconds%3600)//60:.0f}m\"\n",
        "\n",
        "            progress_text = f\"🎨 Generating: {progress_percent:.1f}% complete | \"\n",
        "            progress_text += f\"⏱️ Elapsed: {format_time(elapsed_time)} | \"\n",
        "            progress_text += f\"⏳ Remaining: {format_time(remaining_time)}\"\n",
        "\n",
        "            # Update progress through callback\n",
        "            self.progress_callback(progress_text)\n",
        "\n",
        "    def load_model(self, model_id=\"runwayml/stable-diffusion-v1-5\", progress=gr.Progress()):\n",
        "        \"\"\"Load the Stable Diffusion model with memory optimizations and progress tracking\"\"\"\n",
        "        try:\n",
        "            # Progress tracking for model loading\n",
        "            progress(0.1, desc=\"🔄 Clearing previous models...\")\n",
        "\n",
        "            # Clear any existing model from memory\n",
        "            if self.pipe is not None:\n",
        "                del self.pipe\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "            progress(0.3, desc=\"📥 Downloading model (this may take a while for first time)...\")\n",
        "            print(f\"Loading model: {model_id}\")\n",
        "\n",
        "            # Load model with memory optimizations\n",
        "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                use_safetensors=True,\n",
        "                variant=\"fp16\" if self.device == \"cuda\" else None,\n",
        "                safety_checker=None,  # Disable safety checker to save memory\n",
        "                requires_safety_checker=False\n",
        "            )\n",
        "\n",
        "            progress(0.7, desc=\"⚙️ Optimizing model for Colab...\")\n",
        "\n",
        "            # Use faster, more memory-efficient scheduler\n",
        "            self.pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
        "                self.pipe.scheduler.config\n",
        "            )\n",
        "\n",
        "            # Enable all memory optimizations\n",
        "            if self.device == \"cuda\":\n",
        "                self.pipe.enable_attention_slicing(1)  # More aggressive slicing\n",
        "                self.pipe.enable_vae_slicing()\n",
        "                self.pipe.enable_vae_tiling()  # Additional VAE optimization\n",
        "\n",
        "            progress(0.9, desc=\"🚀 Moving model to device...\")\n",
        "            self.pipe = self.pipe.to(self.device)\n",
        "\n",
        "            # Enable CPU offloading to save VRAM\n",
        "            if self.device == \"cuda\":\n",
        "                self.pipe.enable_model_cpu_offload()\n",
        "\n",
        "            progress(1.0, desc=\"✅ Model loaded successfully!\")\n",
        "\n",
        "            # Clear cache after loading\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(\"Model loaded successfully!\")\n",
        "            return \"✅ Model loaded successfully! Ready to generate images.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"❌ Error loading model: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            return error_msg\n",
        "\n",
        "    def generate_image(self, prompt, negative_prompt=\"\", num_inference_steps=15,\n",
        "                      guidance_scale=7.5, width=512, height=512, seed=-1,\n",
        "                      progress=gr.Progress()):\n",
        "        \"\"\"Generate image from text prompt with real-time progress\"\"\"\n",
        "\n",
        "        if self.pipe is None:\n",
        "            return None, \"❌ Please load a model first!\"\n",
        "\n",
        "        try:\n",
        "            # Validate inputs for optimal Colab performance\n",
        "            if width > 768 or height > 768:\n",
        "                return None, \"❌ Image dimensions too large for Colab! Keep width and height ≤ 768px\"\n",
        "\n",
        "            if num_inference_steps > 30:\n",
        "                return None, \"❌ Too many inference steps! Keep steps ≤ 30 for faster generation\"\n",
        "\n",
        "            # Set up progress tracking\n",
        "            self.total_steps = num_inference_steps\n",
        "            self.start_time = time.time()\n",
        "\n",
        "            # Progress callback for Gradio\n",
        "            def update_progress(text):\n",
        "                progress(None, desc=text)\n",
        "\n",
        "            self.progress_callback = update_progress\n",
        "\n",
        "            progress(0.05, desc=\"🎲 Setting up generation parameters...\")\n",
        "\n",
        "            # Set random seed if specified\n",
        "            if seed != -1:\n",
        "                torch.manual_seed(seed)\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.manual_seed(seed)\n",
        "\n",
        "            progress(0.1, desc=\"🧹 Clearing memory cache...\")\n",
        "\n",
        "            # Clear cache before generation\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            progress(0.15, desc=f\"🎨 Starting image generation ({num_inference_steps} steps)...\")\n",
        "            print(f\"Generating image for prompt: '{prompt}'\")\n",
        "\n",
        "            # Generate image with progress callback\n",
        "            with torch.autocast(self.device):\n",
        "                result = self.pipe(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    width=width,\n",
        "                    height=height,\n",
        "                    callback=self.progress_fn,\n",
        "                    callback_steps=1  # Update progress every step\n",
        "                )\n",
        "\n",
        "            image = result.images[0]\n",
        "\n",
        "            progress(0.95, desc=\"🖼️ Finalizing image...\")\n",
        "\n",
        "            # Clear cache after generation\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            total_time = time.time() - self.start_time\n",
        "            status_msg = f\"✅ Image generated successfully in {total_time:.1f}s!\"\n",
        "            progress(1.0, desc=status_msg)\n",
        "            print(status_msg)\n",
        "            return image, status_msg\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"❌ Error generating image: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            progress(1.0, desc=error_msg)\n",
        "            return None, error_msg\n",
        "\n",
        "    def get_memory_info(self):\n",
        "        \"\"\"Get current memory usage information\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            return f\"GPU Memory - Used: {allocated:.2f}GB | Reserved: {reserved:.2f}GB | Total: {total:.1f}GB\"\n",
        "        else:\n",
        "            return \"Running on CPU\""
      ],
      "metadata": {
        "id": "GticBEhl9q2g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🖼️ `StableDiffusionGenerator` Class Explained\n",
        "\n",
        "This class is a **wrapper around the Stable Diffusion pipeline** that makes it easy to:\n",
        "\n",
        "- load and optimize a model for Colab,\n",
        "- generate images from text prompts,\n",
        "- track real-time progress,\n",
        "- manage GPU memory usage.\n",
        "\n",
        "It combines utilities from `diffusers`, `torch`, and `gradio` into one organized interface.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `__init__`\n",
        "**Purpose:** Class initializer (constructor).  \n",
        "- Sets up the device (`cuda` if available, else CPU).  \n",
        "- Initializes placeholders for the pipeline (`self.pipe`) and progress tracking variables.  \n",
        "\n",
        "**In this code:**\n",
        "- `self.pipe = None` → pipeline not yet loaded.  \n",
        "- `self.device` → auto-detects GPU (important for Colab).  \n",
        "- `self.progress_callback` → stores a function to update progress in the UI.  \n",
        "- `self.start_time` + `self.total_steps` → used to estimate how long image generation will take.  \n",
        "- Prints which device is being used.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `progress_fn(self, step, timestep, latents)`\n",
        "**Purpose:** A callback function that tracks progress while the model generates images.  \n",
        "- Called every generation step by the diffusion pipeline.  \n",
        "- Calculates elapsed time, estimated remaining time, and % complete.  \n",
        "- Formats times nicely into seconds, minutes, or hours.  \n",
        "- Sends updates to the Gradio UI via `self.progress_callback`.\n",
        "\n",
        "**In this code:**\n",
        "- Gives real-time feedback like:  \n",
        "  `\"🎨 Generating: 46.7% complete | ⏱️ Elapsed: 12.3s | ⏳ Remaining: 14.5s\"`  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `load_model(self, model_id=..., progress=...)`\n",
        "**Purpose:** Load the Stable Diffusion model with Colab optimizations.  \n",
        "- Clears old models from memory (to prevent VRAM leaks).  \n",
        "- Downloads the specified model from Hugging Face (`from_pretrained`).  \n",
        "- Configures precision (FP16 on GPU, FP32 on CPU).  \n",
        "- Disables the safety checker (saves VRAM).  \n",
        "- Switches scheduler to `EulerDiscreteScheduler` (fast & efficient).  \n",
        "- Enables optimizations: attention slicing, VAE slicing/tiling, CPU offload.  \n",
        "- Moves model to GPU/CPU.  \n",
        "- Returns a success or error message.\n",
        "\n",
        "**In this code:**\n",
        "- Default model: `\"runwayml/stable-diffusion-v1-5\"` (fast & stable).  \n",
        "- Progress messages update in Gradio during each stage of loading.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `generate_image(self, prompt, negative_prompt=\"\", ...)`\n",
        "**Purpose:** Generate an image from a text prompt.  \n",
        "- Validates input (keeps resolution ≤ 768px, steps ≤ 30).  \n",
        "- Sets up progress tracking (start time, total steps).  \n",
        "- Applies seed if specified (for reproducibility).  \n",
        "- Clears GPU memory before generation.  \n",
        "- Calls `self.pipe(...)` to run Stable Diffusion with the prompt.  \n",
        "- Uses `self.progress_fn` to update status every step.  \n",
        "- Collects the output image, clears cache again.  \n",
        "- Returns `(image, status_message)`.\n",
        "\n",
        "**In this code:**\n",
        "- Default settings:  \n",
        "  - 15 steps,  \n",
        "  - 512x512 resolution,  \n",
        "  - guidance scale 7.5 (how strongly the image follows the prompt).  \n",
        "- Example message after success:  \n",
        "  `\"✅ Image generated successfully in 23.4s!\"`  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 `get_memory_info(self)`\n",
        "**Purpose:** Report current GPU memory usage.  \n",
        "- Uses PyTorch APIs (`torch.cuda.memory_allocated`, `torch.cuda.memory_reserved`).  \n",
        "- Reports how much GPU memory is used, reserved, and the total available.  \n",
        "- If running on CPU, just returns `\"Running on CPU\"`.  \n",
        "\n",
        "**In this code:**\n",
        "- Helpful for monitoring VRAM in Colab.  \n",
        "- Example: `\"GPU Memory - Used: 2.45GB | Reserved: 3.10GB | Total: 15.8GB\"`  \n",
        "\n",
        "---\n",
        "\n",
        "# ✅ Summary\n",
        "This class is a **Colab-optimized Stable Diffusion manager**:\n",
        "- `__init__` → sets up device and variables.  \n",
        "- `progress_fn` → tracks real-time progress.  \n",
        "- `load_model` → loads and optimizes a model.  \n",
        "- `generate_image` → generates images with progress feedback.  \n",
        "- `get_memory_info` → reports GPU/CPU memory usage.  \n"
      ],
      "metadata": {
        "id": "lxJDPO01FYoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the generator\n",
        "generator = StableDiffusionGenerator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn8MVpv39rCd",
        "outputId": "c163d682-a0c0-4721-e2f8-a9a4aca28b45"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔹 `generator = StableDiffusionGenerator()`\n",
        "\n",
        "**Purpose:**  \n",
        "This line **creates an instance of my `StableDiffusionGenerator` class**, which I defined earlier.  \n",
        "It’s like turning the blueprint (class) into a usable object that can load models, generate images, and report memory usage.\n",
        "\n",
        "---\n",
        "\n",
        "**What happens when this runs:**\n",
        "- Calls the `__init__` method of the class automatically.  \n",
        "- Sets up:\n",
        "  - `self.pipe = None` → pipeline not loaded yet.  \n",
        "  - `self.device` → checks if CUDA GPU is available, otherwise falls back to CPU.  \n",
        "  - `self.progress_callback`, `self.start_time`, `self.total_steps` → initialized to manage progress tracking.  \n",
        "- Prints out which device is being used:  \n",
        "  - Example: `\"Using device: cuda\"` in Colab with GPU.  \n",
        "  - Example: `\"Using device: cpu\"` if no GPU available.  \n",
        "\n",
        "---\n",
        "\n",
        "**In my code:**  \n",
        "- `generator` is the **main object** I interact with in later cells.  \n",
        "- I call:  \n",
        "  - `generator.load_model(...)` → to load Stable Diffusion.  \n",
        "  - `generator.generate_image(...)` → to generate images.  \n",
        "  - `generator.get_memory_info()` → to check GPU memory usage.  \n",
        "- Without this line, none of the class methods could be used."
      ],
      "metadata": {
        "id": "uuUZ6hAyId0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Fast Stable Diffusion Generator\", theme=gr.themes.Soft()) as interface:\n",
        "\n",
        "        gr.Markdown(\"# ⚡ Fast Stable Diffusion Text-to-Image Generator\")\n",
        "        gr.Markdown(\"**Optimized for Google Colab with Real-time Progress Tracking**\")\n",
        "\n",
        "        with gr.Tab(\"🎨 Image Generation\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # Model loading section\n",
        "                    gr.Markdown(\"### 1. Load Model\")\n",
        "                    with gr.Row():\n",
        "                        model_dropdown = gr.Dropdown(\n",
        "                            choices=[\n",
        "                                \"runwayml/stable-diffusion-v1-5\",  # Fastest, most stable\n",
        "                                \"stabilityai/stable-diffusion-2-1-base\",  # Good balance\n",
        "                                \"CompVis/stable-diffusion-v1-4\"  # Alternative option\n",
        "                            ],\n",
        "                            value=\"runwayml/stable-diffusion-v1-5\",\n",
        "                            label=\"Select Model (v1-5 is fastest)\"\n",
        "                        )\n",
        "                        load_btn = gr.Button(\"🚀 Load Model\", variant=\"primary\", size=\"sm\")\n",
        "\n",
        "                    load_status = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "                    gr.Markdown(\"### 2. Generation Settings\")\n",
        "                    prompt = gr.Textbox(\n",
        "                        label=\"✨ Prompt\",\n",
        "                        placeholder=\"a beautiful sunset over mountains, highly detailed, cinematic lighting\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                    negative_prompt = gr.Textbox(\n",
        "                        label=\"🚫 Negative Prompt (Optional)\",\n",
        "                        placeholder=\"blurry, bad quality, distorted, ugly\",\n",
        "                        lines=2\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        steps = gr.Slider(\n",
        "                            minimum=8, maximum=30, value=15, step=1,\n",
        "                            label=\"⚡ Inference Steps (15 recommended)\",\n",
        "                            info=\"Fewer steps = faster generation\"\n",
        "                        )\n",
        "                        guidance = gr.Slider(\n",
        "                            minimum=3, maximum=15, value=7.5, step=0.5,\n",
        "                            label=\"🎯 Guidance Scale\",\n",
        "                            info=\"Higher = follows prompt more closely\"\n",
        "                        )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        width = gr.Dropdown(\n",
        "                            choices=[256, 384, 512, 640, 768],\n",
        "                            value=512,\n",
        "                            label=\"📐 Width (px)\"\n",
        "                        )\n",
        "                        height = gr.Dropdown(\n",
        "                            choices=[256, 384, 512, 640, 768],\n",
        "                            value=512,\n",
        "                            label=\"📐 Height (px)\"\n",
        "                        )\n",
        "\n",
        "                    seed = gr.Number(\n",
        "                        label=\"🎲 Seed (-1 for random)\",\n",
        "                        value=-1,\n",
        "                        precision=0,\n",
        "                        info=\"Same seed = same image\"\n",
        "                    )\n",
        "\n",
        "                    generate_btn = gr.Button(\n",
        "                        \"🎨 Generate Image\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\"\n",
        "                    )\n",
        "\n",
        "                    # Quick presets\n",
        "                    gr.Markdown(\"### 🚀 Quick Presets\")\n",
        "                    with gr.Row():\n",
        "                        fast_btn = gr.Button(\"⚡ Ultra Fast (8 steps)\", size=\"sm\")\n",
        "                        balanced_btn = gr.Button(\"⚖️ Balanced (15 steps)\", size=\"sm\")\n",
        "                        quality_btn = gr.Button(\"🎨 High Quality (25 steps)\", size=\"sm\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### 🖼️ Generated Image\")\n",
        "                    output_image = gr.Image(label=\"Generated Image\", type=\"pil\", height=400)\n",
        "                    generation_status = gr.Textbox(label=\"📊 Generation Status\", interactive=False)\n",
        "\n",
        "                    # Time estimation display\n",
        "                    gr.Markdown(\"### ⏱️ Expected Generation Times\")\n",
        "                    gr.HTML(\"\"\"\n",
        "                    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                                padding: 15px; border-radius: 10px; color: white;\">\n",
        "                        <b>Colab Performance (GPU):</b><br>\n",
        "                        • 8 steps: ~10-15 seconds<br>\n",
        "                        • 15 steps: ~20-30 seconds<br>\n",
        "                        • 25 steps: ~40-60 seconds<br>\n",
        "                        <small><i>512x512 resolution</i></small>\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "\n",
        "        with gr.Tab(\"📊 System Info\"):\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### 💻 System Information\")\n",
        "                memory_info = gr.Textbox(label=\"Memory Usage\", interactive=False)\n",
        "                refresh_memory_btn = gr.Button(\"🔄 Refresh Memory Info\")\n",
        "\n",
        "                gr.Markdown(\"### ⚡ Optimization Tips\")\n",
        "                gr.HTML(\"\"\"\n",
        "                <div style=\"background: #f0f8ff; padding: 20px; border-radius: 10px; border-left: 4px solid #4CAF50;\">\n",
        "                    <h4>🚀 For Fastest Generation:</h4>\n",
        "                    <ul>\n",
        "                        <li><b>Use 8-15 inference steps</b> (sweet spot for speed/quality)</li>\n",
        "                        <li><b>Keep resolution at 512x512</b> (perfect for most uses)</li>\n",
        "                        <li><b>Use runwayml/stable-diffusion-v1-5</b> (fastest model)</li>\n",
        "                        <li><b>Enable GPU in Colab</b> (Runtime → Change runtime type → GPU)</li>\n",
        "                        <li><b>Clear memory between generations</b> (automatic in this interface)</li>\n",
        "                    </ul>\n",
        "\n",
        "                    <h4>🎨 For Best Quality:</h4>\n",
        "                    <ul>\n",
        "                        <li><b>Use detailed, descriptive prompts</b></li>\n",
        "                        <li><b>Add style keywords:</b> \"highly detailed\", \"cinematic\", \"4k\"</li>\n",
        "                        <li><b>Use negative prompts</b> to avoid unwanted elements</li>\n",
        "                        <li><b>Guidance scale 7-8</b> works best for most images</li>\n",
        "                    </ul>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "        # Event handlers\n",
        "        load_btn.click(\n",
        "            fn=generator.load_model,\n",
        "            inputs=[model_dropdown],\n",
        "            outputs=[load_status],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        generate_btn.click(\n",
        "            fn=generator.generate_image,\n",
        "            inputs=[prompt, negative_prompt, steps, guidance, width, height, seed],\n",
        "            outputs=[output_image, generation_status],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        # Quick preset handlers\n",
        "        def set_ultra_fast():\n",
        "            return 8, 7.0, 512, 512\n",
        "\n",
        "        def set_balanced():\n",
        "            return 15, 7.5, 512, 512\n",
        "\n",
        "        def set_high_quality():\n",
        "            return 25, 8.0, 640, 640\n",
        "\n",
        "        fast_btn.click(\n",
        "            fn=set_ultra_fast,\n",
        "            outputs=[steps, guidance, width, height]\n",
        "        )\n",
        "\n",
        "        balanced_btn.click(\n",
        "            fn=set_balanced,\n",
        "            outputs=[steps, guidance, width, height]\n",
        "        )\n",
        "\n",
        "        quality_btn.click(\n",
        "            fn=set_high_quality,\n",
        "            outputs=[steps, guidance, width, height]\n",
        "        )\n",
        "\n",
        "        refresh_memory_btn.click(\n",
        "            fn=generator.get_memory_info,\n",
        "            outputs=[memory_info]\n",
        "        )\n",
        "\n",
        "        # Load initial memory info\n",
        "        interface.load(\n",
        "            fn=generator.get_memory_info,\n",
        "            outputs=[memory_info]\n",
        "        )\n",
        "\n",
        "    return interface"
      ],
      "metadata": {
        "id": "iHAO5PBW9rFV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎛️ `create_interface()` – Building the Gradio UI\n",
        "\n",
        "**Purpose:**  \n",
        "This function defines and returns my **Gradio interface**.  \n",
        "It provides a **web-based GUI** for loading models, entering prompts, tuning settings, and generating images.  \n",
        "I don’t interact directly with the class methods anymore — instead, I use buttons, sliders, and textboxes.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 What happens step by step:\n",
        "\n",
        "1. **Interface Setup**\n",
        "   - I use `gr.Blocks` to build a flexible UI layout.\n",
        "   - Title: `\"Fast Stable Diffusion Generator\"`.\n",
        "   - Theme: `gr.themes.Soft()` → makes the UI visually cleaner.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Header**\n",
        "   - Adds a Markdown title and description:\n",
        "     - `\"# ⚡ Fast Stable Diffusion Text-to-Image Generator\"`\n",
        "     - `\"Optimized for Google Colab with Real-time Progress Tracking\"`\n",
        "\n",
        "---\n",
        "\n",
        "3. **🎨 Image Generation Tab**\n",
        "   - Split into **two main columns**:\n",
        "     - **Left Column (controls):**\n",
        "       - **Model loading:**\n",
        "         - Dropdown → select model version (`v1-5`, `2-1-base`, `v1-4`).\n",
        "         - Load button → calls `generator.load_model(...)`.\n",
        "         - Status textbox → shows loading results.\n",
        "       - **Prompt settings:**\n",
        "         - Prompt box (`✨ Prompt`) → main text input.\n",
        "         - Negative prompt box (`🚫 Negative Prompt`) → optional filter for bad outputs.\n",
        "       - **Generation parameters:**\n",
        "         - Slider: `Inference Steps` (8–30).\n",
        "         - Slider: `Guidance Scale` (3–15).\n",
        "         - Dropdowns: `Width` + `Height` (256–768).\n",
        "         - Seed: number input (default -1 for random).\n",
        "       - **Generate button**:\n",
        "         - `\"🎨 Generate Image\"` → runs `generator.generate_image(...)`.\n",
        "       - **Quick Presets**:\n",
        "         - Buttons for **Ultra Fast (8 steps)**, **Balanced (15 steps)**, **High Quality (25 steps)** → auto-adjust sliders.\n",
        "\n",
        "     - **Right Column (outputs):**\n",
        "       - `Generated Image` → shows the output.\n",
        "       - `Generation Status` → shows progress messages.\n",
        "       - **Expected generation times** (pre-calculated HTML info box).\n",
        "\n",
        "---\n",
        "\n",
        "4. **📊 System Info Tab**\n",
        "   - Displays **system + optimization tips**:\n",
        "     - `Memory Usage` box → updated with `generator.get_memory_info()`.\n",
        "     - `Refresh` button → reloads memory info.\n",
        "     - HTML panel → tips for speed vs quality:\n",
        "       - Use **8–15 steps** for speed.\n",
        "       - Keep resolution at **512×512** for efficiency.\n",
        "       - Enable GPU in Colab.\n",
        "\n",
        "---\n",
        "\n",
        "5. **Event Handlers**\n",
        "   - Connects UI elements to class functions:\n",
        "     - `load_btn.click(...)` → calls `generator.load_model`.\n",
        "     - `generate_btn.click(...)` → calls `generator.generate_image`.\n",
        "     - Preset buttons (`fast_btn`, `balanced_btn`, `quality_btn`) → set sliders automatically.\n",
        "     - `refresh_memory_btn.click(...)` → updates memory info.\n",
        "     - `interface.load(...)` → shows memory usage immediately on startup.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 In my code:\n",
        "- This function **ties everything together**:\n",
        "  - `generator.load_model` and `generator.generate_image` are now accessible via buttons.  \n",
        "  - I can **switch models, generate images, and track GPU memory** from a clean UI.  \n",
        "- When I call `create_interface()`, it returns the **Gradio app**, which I then launch with `.launch()` in Colab."
      ],
      "metadata": {
        "id": "jZKn6jeYI2y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create and launch the interface\n",
        "    interface = create_interface()\n",
        "\n",
        "    # Launch with optimized settings for Colab\n",
        "    interface.launch(\n",
        "        share=True,  # Creates public link\n",
        "        inbrowser=True,  # Opens in browser\n",
        "        show_error=True,  # Show errors in interface\n",
        "        quiet=False  # Show progress in console\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "FIKEPVC89rIB",
        "outputId": "7991d044-870e-43a5-b744-7981cca2264a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7d3b2a7e15204ef829.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7d3b2a7e15204ef829.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2Ceq0U19rKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfwNmweg9rNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DASPMxe39rQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}