{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MT0RJ5aozVVR",
        "outputId": "59f9a295-829a-4f93-8a55-b193d2c741e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.34.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers transformers accelerate torch torchvision\n",
        "!pip install gradio safetensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BrVsVj-rzX_B",
        "outputId": "35e2ae06-5450-4102-feff-37eab6eb43f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stable Diffusion Text-to-Image Generator with Gradio UI\n",
        "# Optimized for Google Colab with Real-time Progress Tracking\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import torch\n",
        "import gradio as gr\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, EulerDiscreteScheduler\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class StableDiffusionGenerator:\n",
        "    def __init__(self):\n",
        "        self.pipe = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.progress_callback = None\n",
        "        self.start_time = None\n",
        "        self.total_steps = 0\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def progress_fn(self, step, timestep, latents):\n",
        "        \"\"\"Callback function to track generation progress\"\"\"\n",
        "        if self.progress_callback and self.start_time:\n",
        "            elapsed_time = time.time() - self.start_time\n",
        "            progress_percent = (step / self.total_steps) * 100\n",
        "\n",
        "            if step > 0:  # Avoid division by zero\n",
        "                estimated_total_time = elapsed_time * self.total_steps / step\n",
        "                remaining_time = estimated_total_time - elapsed_time\n",
        "            else:\n",
        "                remaining_time = 0\n",
        "\n",
        "            # Format time nicely\n",
        "            def format_time(seconds):\n",
        "                if seconds < 60:\n",
        "                    return f\"{seconds:.1f}s\"\n",
        "                elif seconds < 3600:\n",
        "                    return f\"{seconds//60:.0f}m {seconds%60:.0f}s\"\n",
        "                else:\n",
        "                    return f\"{seconds//3600:.0f}h {(seconds%3600)//60:.0f}m\"\n",
        "\n",
        "            progress_text = f\"🎨 Generating: {progress_percent:.1f}% complete | \"\n",
        "            progress_text += f\"⏱️ Elapsed: {format_time(elapsed_time)} | \"\n",
        "            progress_text += f\"⏳ Remaining: {format_time(remaining_time)}\"\n",
        "\n",
        "            # Update progress through callback\n",
        "            self.progress_callback(progress_text)\n",
        "\n",
        "    def load_model(self, model_id=\"runwayml/stable-diffusion-v1-5\", progress=gr.Progress()):\n",
        "        \"\"\"Load the Stable Diffusion model with memory optimizations and progress tracking\"\"\"\n",
        "        try:\n",
        "            # Progress tracking for model loading\n",
        "            progress(0.1, desc=\"🔄 Clearing previous models...\")\n",
        "\n",
        "            # Clear any existing model from memory\n",
        "            if self.pipe is not None:\n",
        "                del self.pipe\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "            progress(0.3, desc=\"📥 Downloading model (this may take a while for first time)...\")\n",
        "            print(f\"Loading model: {model_id}\")\n",
        "\n",
        "            # Load model with memory optimizations\n",
        "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                use_safetensors=True,\n",
        "                variant=\"fp16\" if self.device == \"cuda\" else None,\n",
        "                safety_checker=None,  # Disable safety checker to save memory\n",
        "                requires_safety_checker=False\n",
        "            )\n",
        "\n",
        "            progress(0.7, desc=\"⚙️ Optimizing model for Colab...\")\n",
        "\n",
        "            # Use faster, more memory-efficient scheduler\n",
        "            self.pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
        "                self.pipe.scheduler.config\n",
        "            )\n",
        "\n",
        "            # Enable all memory optimizations\n",
        "            if self.device == \"cuda\":\n",
        "                self.pipe.enable_attention_slicing(1)  # More aggressive slicing\n",
        "                self.pipe.enable_vae_slicing()\n",
        "                self.pipe.enable_vae_tiling()  # Additional VAE optimization\n",
        "\n",
        "            progress(0.9, desc=\"🚀 Moving model to device...\")\n",
        "            self.pipe = self.pipe.to(self.device)\n",
        "\n",
        "            # Enable CPU offloading to save VRAM\n",
        "            if self.device == \"cuda\":\n",
        "                self.pipe.enable_model_cpu_offload()\n",
        "\n",
        "            progress(1.0, desc=\"✅ Model loaded successfully!\")\n",
        "\n",
        "            # Clear cache after loading\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(\"Model loaded successfully!\")\n",
        "            return \"✅ Model loaded successfully! Ready to generate images.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"❌ Error loading model: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            return error_msg\n",
        "\n",
        "    def generate_image(self, prompt, negative_prompt=\"\", num_inference_steps=15,\n",
        "                      guidance_scale=7.5, width=512, height=512, seed=-1,\n",
        "                      progress=gr.Progress()):\n",
        "        \"\"\"Generate image from text prompt with real-time progress\"\"\"\n",
        "\n",
        "        if self.pipe is None:\n",
        "            return None, \"❌ Please load a model first!\"\n",
        "\n",
        "        try:\n",
        "            # Validate inputs for optimal Colab performance\n",
        "            if width > 768 or height > 768:\n",
        "                return None, \"❌ Image dimensions too large for Colab! Keep width and height ≤ 768px\"\n",
        "\n",
        "            if num_inference_steps > 30:\n",
        "                return None, \"❌ Too many inference steps! Keep steps ≤ 30 for faster generation\"\n",
        "\n",
        "            # Set up progress tracking\n",
        "            self.total_steps = num_inference_steps\n",
        "            self.start_time = time.time()\n",
        "\n",
        "            # Progress callback for Gradio\n",
        "            def update_progress(text):\n",
        "                progress(None, desc=text)\n",
        "\n",
        "            self.progress_callback = update_progress\n",
        "\n",
        "            progress(0.05, desc=\"🎲 Setting up generation parameters...\")\n",
        "\n",
        "            # Set random seed if specified\n",
        "            if seed != -1:\n",
        "                torch.manual_seed(seed)\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.manual_seed(seed)\n",
        "\n",
        "            progress(0.1, desc=\"🧹 Clearing memory cache...\")\n",
        "\n",
        "            # Clear cache before generation\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            progress(0.15, desc=f\"🎨 Starting image generation ({num_inference_steps} steps)...\")\n",
        "            print(f\"Generating image for prompt: '{prompt}'\")\n",
        "\n",
        "            # Generate image with progress callback\n",
        "            with torch.autocast(self.device):\n",
        "                result = self.pipe(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    width=width,\n",
        "                    height=height,\n",
        "                    callback=self.progress_fn,\n",
        "                    callback_steps=1  # Update progress every step\n",
        "                )\n",
        "\n",
        "            image = result.images[0]\n",
        "\n",
        "            progress(0.95, desc=\"🖼️ Finalizing image...\")\n",
        "\n",
        "            # Clear cache after generation\n",
        "            if self.device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            total_time = time.time() - self.start_time\n",
        "            status_msg = f\"✅ Image generated successfully in {total_time:.1f}s!\"\n",
        "            progress(1.0, desc=status_msg)\n",
        "            print(status_msg)\n",
        "            return image, status_msg\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"❌ Error generating image: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            progress(1.0, desc=error_msg)\n",
        "            return None, error_msg\n",
        "\n",
        "    def get_memory_info(self):\n",
        "        \"\"\"Get current memory usage information\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            return f\"GPU Memory - Used: {allocated:.2f}GB | Reserved: {reserved:.2f}GB | Total: {total:.1f}GB\"\n",
        "        else:\n",
        "            return \"Running on CPU\"\n",
        "\n",
        "# Initialize the generator\n",
        "generator = StableDiffusionGenerator()\n",
        "\n",
        "# Define the Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Fast Stable Diffusion Generator\", theme=gr.themes.Soft()) as interface:\n",
        "\n",
        "        gr.Markdown(\"# ⚡ Fast Stable Diffusion Text-to-Image Generator\")\n",
        "        gr.Markdown(\"**Optimized for Google Colab with Real-time Progress Tracking**\")\n",
        "\n",
        "        with gr.Tab(\"🎨 Image Generation\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    # Model loading section\n",
        "                    gr.Markdown(\"### 1. Load Model\")\n",
        "                    with gr.Row():\n",
        "                        model_dropdown = gr.Dropdown(\n",
        "                            choices=[\n",
        "                                \"runwayml/stable-diffusion-v1-5\",  # Fastest, most stable\n",
        "                                \"stabilityai/stable-diffusion-2-1-base\",  # Good balance\n",
        "                                \"CompVis/stable-diffusion-v1-4\"  # Alternative option\n",
        "                            ],\n",
        "                            value=\"runwayml/stable-diffusion-v1-5\",\n",
        "                            label=\"Select Model (v1-5 is fastest)\"\n",
        "                        )\n",
        "                        load_btn = gr.Button(\"🚀 Load Model\", variant=\"primary\", size=\"sm\")\n",
        "\n",
        "                    load_status = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "                    gr.Markdown(\"### 2. Generation Settings\")\n",
        "                    prompt = gr.Textbox(\n",
        "                        label=\"✨ Prompt\",\n",
        "                        placeholder=\"a beautiful sunset over mountains, highly detailed, cinematic lighting\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                    negative_prompt = gr.Textbox(\n",
        "                        label=\"🚫 Negative Prompt (Optional)\",\n",
        "                        placeholder=\"blurry, bad quality, distorted, ugly\",\n",
        "                        lines=2\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        steps = gr.Slider(\n",
        "                            minimum=8, maximum=30, value=15, step=1,\n",
        "                            label=\"⚡ Inference Steps (15 recommended)\",\n",
        "                            info=\"Fewer steps = faster generation\"\n",
        "                        )\n",
        "                        guidance = gr.Slider(\n",
        "                            minimum=3, maximum=15, value=7.5, step=0.5,\n",
        "                            label=\"🎯 Guidance Scale\",\n",
        "                            info=\"Higher = follows prompt more closely\"\n",
        "                        )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        width = gr.Dropdown(\n",
        "                            choices=[256, 384, 512, 640, 768],\n",
        "                            value=512,\n",
        "                            label=\"📐 Width (px)\"\n",
        "                        )\n",
        "                        height = gr.Dropdown(\n",
        "                            choices=[256, 384, 512, 640, 768],\n",
        "                            value=512,\n",
        "                            label=\"📐 Height (px)\"\n",
        "                        )\n",
        "\n",
        "                    seed = gr.Number(\n",
        "                        label=\"🎲 Seed (-1 for random)\",\n",
        "                        value=-1,\n",
        "                        precision=0,\n",
        "                        info=\"Same seed = same image\"\n",
        "                    )\n",
        "\n",
        "                    generate_btn = gr.Button(\n",
        "                        \"🎨 Generate Image\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\"\n",
        "                    )\n",
        "\n",
        "                    # Quick presets\n",
        "                    gr.Markdown(\"### 🚀 Quick Presets\")\n",
        "                    with gr.Row():\n",
        "                        fast_btn = gr.Button(\"⚡ Ultra Fast (8 steps)\", size=\"sm\")\n",
        "                        balanced_btn = gr.Button(\"⚖️ Balanced (15 steps)\", size=\"sm\")\n",
        "                        quality_btn = gr.Button(\"🎨 High Quality (25 steps)\", size=\"sm\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### 🖼️ Generated Image\")\n",
        "                    output_image = gr.Image(label=\"Generated Image\", type=\"pil\", height=400)\n",
        "                    generation_status = gr.Textbox(label=\"📊 Generation Status\", interactive=False)\n",
        "\n",
        "                    # Time estimation display\n",
        "                    gr.Markdown(\"### ⏱️ Expected Generation Times\")\n",
        "                    gr.HTML(\"\"\"\n",
        "                    <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                                padding: 15px; border-radius: 10px; color: white;\">\n",
        "                        <b>Colab Performance (GPU):</b><br>\n",
        "                        • 8 steps: ~10-15 seconds<br>\n",
        "                        • 15 steps: ~20-30 seconds<br>\n",
        "                        • 25 steps: ~40-60 seconds<br>\n",
        "                        <small><i>512x512 resolution</i></small>\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "\n",
        "        with gr.Tab(\"📊 System Info\"):\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### 💻 System Information\")\n",
        "                memory_info = gr.Textbox(label=\"Memory Usage\", interactive=False)\n",
        "                refresh_memory_btn = gr.Button(\"🔄 Refresh Memory Info\")\n",
        "\n",
        "                gr.Markdown(\"### ⚡ Optimization Tips\")\n",
        "                gr.HTML(\"\"\"\n",
        "                <div style=\"background: #f0f8ff; padding: 20px; border-radius: 10px; border-left: 4px solid #4CAF50;\">\n",
        "                    <h4>🚀 For Fastest Generation:</h4>\n",
        "                    <ul>\n",
        "                        <li><b>Use 8-15 inference steps</b> (sweet spot for speed/quality)</li>\n",
        "                        <li><b>Keep resolution at 512x512</b> (perfect for most uses)</li>\n",
        "                        <li><b>Use runwayml/stable-diffusion-v1-5</b> (fastest model)</li>\n",
        "                        <li><b>Enable GPU in Colab</b> (Runtime → Change runtime type → GPU)</li>\n",
        "                        <li><b>Clear memory between generations</b> (automatic in this interface)</li>\n",
        "                    </ul>\n",
        "\n",
        "                    <h4>🎨 For Best Quality:</h4>\n",
        "                    <ul>\n",
        "                        <li><b>Use detailed, descriptive prompts</b></li>\n",
        "                        <li><b>Add style keywords:</b> \"highly detailed\", \"cinematic\", \"4k\"</li>\n",
        "                        <li><b>Use negative prompts</b> to avoid unwanted elements</li>\n",
        "                        <li><b>Guidance scale 7-8</b> works best for most images</li>\n",
        "                    </ul>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "        # Event handlers\n",
        "        load_btn.click(\n",
        "            fn=generator.load_model,\n",
        "            inputs=[model_dropdown],\n",
        "            outputs=[load_status],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        generate_btn.click(\n",
        "            fn=generator.generate_image,\n",
        "            inputs=[prompt, negative_prompt, steps, guidance, width, height, seed],\n",
        "            outputs=[output_image, generation_status],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        # Quick preset handlers\n",
        "        def set_ultra_fast():\n",
        "            return 8, 7.0, 512, 512\n",
        "\n",
        "        def set_balanced():\n",
        "            return 15, 7.5, 512, 512\n",
        "\n",
        "        def set_high_quality():\n",
        "            return 25, 8.0, 640, 640\n",
        "\n",
        "        fast_btn.click(\n",
        "            fn=set_ultra_fast,\n",
        "            outputs=[steps, guidance, width, height]\n",
        "        )\n",
        "\n",
        "        balanced_btn.click(\n",
        "            fn=set_balanced,\n",
        "            outputs=[steps, guidance, width, height]\n",
        "        )\n",
        "\n",
        "        quality_btn.click(\n",
        "            fn=set_high_quality,\n",
        "            outputs=[steps, guidance, width, height]\n",
        "        )\n",
        "\n",
        "        refresh_memory_btn.click(\n",
        "            fn=generator.get_memory_info,\n",
        "            outputs=[memory_info]\n",
        "        )\n",
        "\n",
        "        # Load initial memory info\n",
        "        interface.load(\n",
        "            fn=generator.get_memory_info,\n",
        "            outputs=[memory_info]\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Optimized installation instructions for Colab\n",
        "def print_installation_instructions():\n",
        "    print(\"=\"*70)\n",
        "    print(\"🚀 FAST SETUP FOR GOOGLE COLAB\")\n",
        "    print(\"=\"*70)\n",
        "    print()\n",
        "    print(\"📋 Copy and run these commands in separate Colab cells:\")\n",
        "    print()\n",
        "    print(\"# ⚙️ Cell 1: Install dependencies (run once)\")\n",
        "    print(\"!pip install -q diffusers[torch] transformers accelerate\")\n",
        "    print(\"!pip install -q gradio safetensors\")\n",
        "    print()\n",
        "    print(\"# 🖥️ Cell 2: Enable GPU and check status\")\n",
        "    print(\"import torch\")\n",
        "    print(\"print('GPU Available:', torch.cuda.is_available())\")\n",
        "    print(\"if torch.cuda.is_available():\")\n",
        "    print(\"    print('GPU Model:', torch.cuda.get_device_name())\")\n",
        "    print(\"    print('GPU Memory:', f'{torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB')\")\n",
        "    print(\"else:\")\n",
        "    print(\"    print('⚠️ GPU not enabled! Go to Runtime > Change runtime type > Hardware accelerator > GPU')\")\n",
        "    print()\n",
        "    print(\"# 🎨 Cell 3: Run the generator\")\n",
        "    print(\"# (paste the main script here)\")\n",
        "    print()\n",
        "    print(\"⏱️ Expected times with optimizations:\")\n",
        "    print(\"   • Model loading: 1-2 minutes (first time only)\")\n",
        "    print(\"   • Image generation: 10-60 seconds (depending on settings)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print_installation_instructions()\n",
        "\n",
        "    # Create and launch the interface\n",
        "    interface = create_interface()\n",
        "\n",
        "    # Launch with optimized settings for Colab\n",
        "    interface.launch(\n",
        "        share=True,  # Creates public link\n",
        "        inbrowser=True,  # Opens in browser\n",
        "        show_error=True,  # Show errors in interface\n",
        "        quiet=False  # Show progress in console\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1060
        },
        "id": "6tbJFy3izZ8_",
        "outputId": "b14826fd-62c2-45c6-b23c-539b875adc74"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "======================================================================\n",
            "🚀 FAST SETUP FOR GOOGLE COLAB\n",
            "======================================================================\n",
            "\n",
            "📋 Copy and run these commands in separate Colab cells:\n",
            "\n",
            "# ⚙️ Cell 1: Install dependencies (run once)\n",
            "!pip install -q diffusers[torch] transformers accelerate\n",
            "!pip install -q gradio safetensors\n",
            "\n",
            "# 🖥️ Cell 2: Enable GPU and check status\n",
            "import torch\n",
            "print('GPU Available:', torch.cuda.is_available())\n",
            "if torch.cuda.is_available():\n",
            "    print('GPU Model:', torch.cuda.get_device_name())\n",
            "    print('GPU Memory:', f'{torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB')\n",
            "else:\n",
            "    print('⚠️ GPU not enabled! Go to Runtime > Change runtime type > Hardware accelerator > GPU')\n",
            "\n",
            "# 🎨 Cell 3: Run the generator\n",
            "# (paste the main script here)\n",
            "\n",
            "⏱️ Expected times with optimizations:\n",
            "   • Model loading: 1-2 minutes (first time only)\n",
            "   • Image generation: 10-60 seconds (depending on settings)\n",
            "======================================================================\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c53f45577ad48d3765.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c53f45577ad48d3765.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}